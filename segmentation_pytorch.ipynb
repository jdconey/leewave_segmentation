{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cosmetic-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage.io import imread\n",
    "from torch.utils import data\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from sklearn.externals._pilutil import bytescale\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "animated-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: jdconey (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">gallant-pine-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jdconey/lee_seg\" target=\"_blank\">https://wandb.ai/jdconey/lee_seg</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jdconey/lee_seg/runs/irq9wktb\" target=\"_blank\">https://wandb.ai/jdconey/lee_seg/runs/irq9wktb</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\mm16jdc\\Documents\\CEDA_satellites\\leewave_segmentation\\wandb\\run-20210322_201625-irq9wktb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(irq9wktb)</h1><iframe src=\"https://wandb.ai/jdconey/lee_seg/runs/irq9wktb\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x27fca9f75c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project=\"lee_seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coupled-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataSet(data.Dataset):\n",
    "    def __init__(self,\n",
    "       #          ibase,mbase,\n",
    "                 inputs: list,\n",
    "                 targets: list,\n",
    "                 transform=None\n",
    "                 ):\n",
    "      #  self.ibase = ibase\n",
    "      #  self.mbase = mbase\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.inputs_dtype = torch.float32\n",
    "        self.targets_dtype = torch.long\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    index: int):\n",
    "        # Select the sample\n",
    "        input_ID = self.inputs[index]\n",
    "        target_ID = self.targets[index]\n",
    "\n",
    "        # Load input and target\n",
    "        x, y = imread(input_ID), imread(target_ID)\n",
    "\n",
    "        # Preprocessing\n",
    "        if self.transform is not None:\n",
    "            x, y = self.transform(x, y)\n",
    "\n",
    "        # Typecasting\n",
    "        x, y = torch.from_numpy(x).type(self.inputs_dtype), torch.from_numpy(y).type(self.targets_dtype)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "widespread-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_target(tar: np.ndarray):\n",
    "    classes = np.unique(tar)\n",
    "    dummy = np.zeros_like(tar)\n",
    "    for idx, value in enumerate(classes):\n",
    "        mask = np.where(tar == value)\n",
    "        dummy[mask] = idx\n",
    "\n",
    "    return dummy\n",
    "\n",
    "\n",
    "def normalize_01(inp: np.ndarray):\n",
    "    inp_out = (inp - np.min(inp)) / np.ptp(inp)\n",
    "    return inp_out\n",
    "\n",
    "\n",
    "def normalize(inp: np.ndarray, mean: float, std: float):\n",
    "    inp_out = (inp - mean) / std\n",
    "    return inp_out\n",
    "\n",
    "\n",
    "def re_normalize(inp: np.ndarray,\n",
    "                 low: int = 0,\n",
    "                 high: int = 255\n",
    "                 ):\n",
    "    \"\"\"Normalize the data to a certain range. Default: [0-255]\"\"\"\n",
    "    inp_out = bytescale(inp, low=low, high=high)\n",
    "    return inp_out\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, inp, target):\n",
    "        for t in self.transforms:\n",
    "            inp, target = t(inp, target)\n",
    "        return inp, target\n",
    "\n",
    "    def __repr__(self): return str([transform for transform in self.transforms])\n",
    "\n",
    "\n",
    "class MoveAxis:\n",
    "    \"\"\"From [H, W, C] to [C, H, W]\"\"\"\n",
    "\n",
    "    def __init__(self, transform_input: bool = True, transform_target: bool = False):\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_target = transform_target\n",
    "\n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        if self.transform_input: inp = np.moveaxis(inp, -1, 0)\n",
    "        if self.transform_target: tar = np.moveaxis(inp, -1, 0)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class DenseTarget:\n",
    "    \"\"\"Creates segmentation maps with consecutive integers, starting from 0\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        tar = create_dense_target(tar)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class Resize:\n",
    "    \"\"\"Resizes the image and target - based on skimage\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size: tuple,\n",
    "                 target_size: tuple,\n",
    "                 input_kwargs: dict = {},\n",
    "                 target_kwargs: dict = {'order': 0, 'anti_aliasing': False, 'preserve_range': True}\n",
    "                 ):\n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        self.input_kwargs = input_kwargs\n",
    "        self.target_kwargs = target_kwargs\n",
    "\n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        self.input_dtype = inp.dtype\n",
    "        self.target_dtype = tar.dtype\n",
    "\n",
    "        inp_out = resize(image=inp,\n",
    "                         output_shape=self.input_size,\n",
    "                         **self.input_kwargs\n",
    "                         )\n",
    "        tar_out = resize(image=tar,\n",
    "                         output_shape=self.target_size,\n",
    "                         **self.target_kwargs\n",
    "                         ).astype(self.target_dtype)\n",
    "        return inp_out, tar_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class Normalize01:\n",
    "    \"\"\"Squash image input to the value range [0, 1] (no clipping)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, inp, tar):\n",
    "        inp = normalize_01(inp)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    \"\"\"Normalize based on mean and standard deviation.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mean: float,\n",
    "                 std: float,\n",
    "                 transform_input=True,\n",
    "                 transform_target=False\n",
    "                 ):\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_target = transform_target\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, inp, tar):\n",
    "        inp = normalize(inp)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class AlbuSeg2d:\n",
    "    def __init__(self, albu):\n",
    "        self.albu = albu\n",
    "\n",
    "    def __call__(self, inp, tar):\n",
    "        # input, target\n",
    "        out_dict = self.albu(image=inp, mask=tar)\n",
    "        input_out = out_dict['image']\n",
    "        target_out = out_dict['mask']\n",
    "\n",
    "        return input_out, target_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swiss-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    DenseTarget(),\n",
    "    MoveAxis(),\n",
    "    Normalize01()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-islam",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-broadway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collectible-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory\n",
    "root = Path('C:/Users/mm16jdc/Documents/CEDA_satellites/pytorch_mask_test/images_3/')\n",
    "\n",
    "def get_filenames_of_path(path: Path, ext: str = '*'):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tamil-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target files\n",
    "train_inputs = get_filenames_of_path(root / 'train/images')\n",
    "train_targets = get_filenames_of_path(root / 'train/masks')\n",
    "valid_inputs = get_filenames_of_path(root/'valid/images/')\n",
    "valid_targets = get_filenames_of_path(root/'valid/masks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-crest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vital-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training transformations and augmentations\n",
    "transforms = Compose([\n",
    "    DenseTarget(),\n",
    "    MoveAxis(),\n",
    "    Normalize01()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spectacular-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset training\n",
    "dataset_train = SegmentationDataSet(inputs=train_inputs,\n",
    "                                    targets=train_targets,\n",
    "                                    transform=transforms)\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet(inputs=valid_inputs,\n",
    "                                    targets=valid_targets,\n",
    "                                    transform=transforms)\n",
    "\n",
    "# dataloader training\n",
    "dataloader_training = DataLoader(dataset=dataset_train,\n",
    "                                 batch_size=16,\n",
    "                                 shuffle=True)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid,\n",
    "                                   batch_size=16,\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "private-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = shape: torch.Size([16, 3, 400, 400]); type: torch.float32\n",
      "x = min: 0.0; max: 1.0\n",
      "y = shape: torch.Size([16, 400, 400]); class: tensor([0, 1]); type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader_training))\n",
    "\n",
    "print(f'x = shape: {x.shape}; type: {x.dtype}')\n",
    "print(f'x = min: {x.min()}; max: {x.max()}')\n",
    "print(f'y = shape: {y.shape}; class: {y.unique()}; type: {y.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floral-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=2, aux_loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-johnson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "universal-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
    "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "                 epochs: int = 100,\n",
    "                 epoch: int = 0,\n",
    "                 notebook: bool = False\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        progressbar = trange(self.epochs, desc='Progress')\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_DataLoader is not None:\n",
    "                self._validate()\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    self.lr_scheduler.batch(self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "            wandb.log({\"epoch\": self.epoch, \"train_loss\": self.training_loss[-1], \"val_loss\": self.validation_loss[-1]}, step=1)\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate\n",
    "\n",
    "    def _train(self):\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            out = self.model(input)['out']  # one forward pass\n",
    "            loss = self.criterion(out, target)  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
    "\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(input)\n",
    "                loss = self.criterion(out, target)\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "\n",
    "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
    "\n",
    "        self.validation_loss.append(np.mean(valid_losses))\n",
    "\n",
    "        batch_iter.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0318617f4884369a41f512ed2fab6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc324a97833240598c81c25b5a6d2b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "# model\n",
    "#model = UNet(in_channels=3,\n",
    "#             out_channels=2,\n",
    "#             n_blocks=4,\n",
    " #            start_filters=32,\n",
    "#             activation='relu',\n",
    "#             normalization='batch',\n",
    "#             conv_mode='same',\n",
    "#             dim=2).to(device)\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_DataLoader=dataloader_training,\n",
    "                  validation_DataLoader=dataloader_validation,\n",
    "                  lr_scheduler=None,\n",
    "                  epochs=10,\n",
    "                  epoch=0,\n",
    "                  notebook=True)\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-democrat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
