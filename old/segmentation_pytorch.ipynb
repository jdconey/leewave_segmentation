{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cosmetic-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage.io import imread\n",
    "from torch.utils import data\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from sklearn.externals._pilutil import bytescale\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "animated-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:322bmo3h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6552<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\mm16jdc\\Documents\\CEDA_satellites\\leewave_segmentation\\wandb\\run-20210323_074040-322bmo3h\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\mm16jdc\\Documents\\CEDA_satellites\\leewave_segmentation\\wandb\\run-20210323_074040-322bmo3h\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">treasured-water-6</strong>: <a href=\"https://wandb.ai/jdconey/lee_seg/runs/322bmo3h\" target=\"_blank\">https://wandb.ai/jdconey/lee_seg/runs/322bmo3h</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:322bmo3h). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">driven-sponge-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jdconey/lee_seg\" target=\"_blank\">https://wandb.ai/jdconey/lee_seg</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jdconey/lee_seg/runs/2xkfgw1a\" target=\"_blank\">https://wandb.ai/jdconey/lee_seg/runs/2xkfgw1a</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\mm16jdc\\Documents\\CEDA_satellites\\leewave_segmentation\\wandb\\run-20210323_074354-2xkfgw1a</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2xkfgw1a)</h1><iframe src=\"https://wandb.ai/jdconey/lee_seg/runs/2xkfgw1a\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x27f810bdfc8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project=\"lee_seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "coupled-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataSet(data.Dataset):\n",
    "    def __init__(self,\n",
    "       #          ibase,mbase,\n",
    "                 inputs: list,\n",
    "                 targets: list,\n",
    "                 transform=None\n",
    "                 ):\n",
    "      #  self.ibase = ibase\n",
    "      #  self.mbase = mbase\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.inputs_dtype = torch.float32\n",
    "        self.targets_dtype = torch.long\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    index: int):\n",
    "        # Select the sample\n",
    "        input_ID = self.inputs[index]\n",
    "        target_ID = self.targets[index]\n",
    "\n",
    "        # Load input and target\n",
    "        x, y = imread(input_ID), imread(target_ID)\n",
    "\n",
    "        # Preprocessing\n",
    "        if self.transform is not None:\n",
    "            x, y = self.transform(x, y)\n",
    "\n",
    "        # Typecasting\n",
    "        x, y = torch.from_numpy(x).type(self.inputs_dtype), torch.from_numpy(y).type(self.targets_dtype)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "widespread-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_target(tar: np.ndarray):\n",
    "    classes = np.unique(tar)\n",
    "    dummy = np.zeros_like(tar)\n",
    "    for idx, value in enumerate(classes):\n",
    "        mask = np.where(tar == value)\n",
    "        dummy[mask] = idx\n",
    "\n",
    "    return dummy\n",
    "\n",
    "\n",
    "def normalize_01(inp: np.ndarray):\n",
    "    inp_out = (inp - np.min(inp)) / np.ptp(inp)\n",
    "    return inp_out\n",
    "\n",
    "\n",
    "def normalize(inp: np.ndarray, mean: float, std: float):\n",
    "    inp_out = (inp - mean) / std\n",
    "    return inp_out\n",
    "\n",
    "\n",
    "def re_normalize(inp: np.ndarray,\n",
    "                 low: int = 0,\n",
    "                 high: int = 255\n",
    "                 ):\n",
    "    \"\"\"Normalize the data to a certain range. Default: [0-255]\"\"\"\n",
    "    inp_out = bytescale(inp, low=low, high=high)\n",
    "    return inp_out\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, inp, target):\n",
    "        for t in self.transforms:\n",
    "            inp, target = t(inp, target)\n",
    "        return inp, target\n",
    "\n",
    "    def __repr__(self): return str([transform for transform in self.transforms])\n",
    "\n",
    "\n",
    "class MoveAxis:\n",
    "    \"\"\"From [H, W, C] to [C, H, W]\"\"\"\n",
    "\n",
    "    def __init__(self, transform_input: bool = True, transform_target: bool = False):\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_target = transform_target\n",
    "\n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        if self.transform_input: inp = np.moveaxis(inp, -1, 0)\n",
    "        if self.transform_target: tar = np.moveaxis(inp, -1, 0)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class DenseTarget:\n",
    "    \"\"\"Creates segmentation maps with consecutive integers, starting from 0\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        tar = create_dense_target(tar)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class Resize:\n",
    "    \"\"\"Resizes the image and target - based on skimage\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size: tuple,\n",
    "                 target_size: tuple,\n",
    "                 input_kwargs: dict = {},\n",
    "                 target_kwargs: dict = {'order': 0, 'anti_aliasing': False, 'preserve_range': True}\n",
    "                 ):\n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        self.input_kwargs = input_kwargs\n",
    "        self.target_kwargs = target_kwargs\n",
    "\n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        self.input_dtype = inp.dtype\n",
    "        self.target_dtype = tar.dtype\n",
    "\n",
    "        inp_out = resize(image=inp,\n",
    "                         output_shape=self.input_size,\n",
    "                         **self.input_kwargs\n",
    "                         )\n",
    "        tar_out = resize(image=tar,\n",
    "                         output_shape=self.target_size,\n",
    "                         **self.target_kwargs\n",
    "                         ).astype(self.target_dtype)\n",
    "        return inp_out, tar_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class Normalize01:\n",
    "    \"\"\"Squash image input to the value range [0, 1] (no clipping)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, inp, tar):\n",
    "        inp = normalize_01(inp)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    \"\"\"Normalize based on mean and standard deviation.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mean: float,\n",
    "                 std: float,\n",
    "                 transform_input=True,\n",
    "                 transform_target=False\n",
    "                 ):\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_target = transform_target\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, inp, tar):\n",
    "        inp = normalize(inp)\n",
    "\n",
    "        return inp, tar\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})\n",
    "\n",
    "\n",
    "class AlbuSeg2d:\n",
    "    def __init__(self, albu):\n",
    "        self.albu = albu\n",
    "\n",
    "    def __call__(self, inp, tar):\n",
    "        # input, target\n",
    "        out_dict = self.albu(image=inp, mask=tar)\n",
    "        input_out = out_dict['image']\n",
    "        target_out = out_dict['mask']\n",
    "\n",
    "        return input_out, target_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({self.__class__.__name__: self.__dict__})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "swiss-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    Resize(input_size=(400,400),target_size=(100,100)),\n",
    "    DenseTarget(),\n",
    "    MoveAxis(),\n",
    "    Normalize01()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-islam",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-broadway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "collectible-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory\n",
    "root = Path('C:/Users/mm16jdc/Documents/CEDA_satellites/pytorch_mask_test/images_3/')\n",
    "\n",
    "def get_filenames_of_path(path: Path, ext: str = '*'):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "tamil-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target files\n",
    "train_inputs = get_filenames_of_path(root / 'train/images')\n",
    "train_targets = get_filenames_of_path(root / 'train/masks')\n",
    "valid_inputs = get_filenames_of_path(root/'valid/images/')\n",
    "valid_targets = get_filenames_of_path(root/'valid/masks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-crest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "vital-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training transformations and augmentations\n",
    "transforms = Compose([\n",
    "    DenseTarget(),\n",
    "    MoveAxis(),\n",
    "    Normalize01()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "spectacular-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset training\n",
    "dataset_train = SegmentationDataSet(inputs=train_inputs,\n",
    "                                    targets=train_targets,\n",
    "                                    transform=transforms)\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet(inputs=valid_inputs,\n",
    "                                    targets=valid_targets,\n",
    "                                    transform=transforms)\n",
    "\n",
    "# dataloader training\n",
    "dataloader_training = DataLoader(dataset=dataset_train,\n",
    "                                 batch_size=16,\n",
    "                                 shuffle=True)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid,\n",
    "                                   batch_size=16,\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "private-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = shape: torch.Size([16, 3, 400, 400]); type: torch.float32\n",
      "x = min: 0.0; max: 1.0\n",
      "y = shape: torch.Size([16, 400, 400]); class: tensor([0, 1]); type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader_training))\n",
    "\n",
    "print(f'x = shape: {x.shape}; type: {x.dtype}')\n",
    "print(f'x = min: {x.min()}; max: {x.max()}')\n",
    "print(f'y = shape: {y.shape}; class: {y.unique()}; type: {y.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "floral-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=2, aux_loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "industrial-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_classes = ['no wave','lee wave']\n",
    "def labels():\n",
    "  l = {}\n",
    "  for i, label in enumerate(segmentation_classes):\n",
    "    l[i] = label\n",
    "  return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "future-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wb_mask(bg_img, pred_mask, true_mask):\n",
    "  return wandb.Image(bg_img, masks={\n",
    "    \"prediction\" : {\"mask_data\" : pred_mask, \"class_labels\" : labels()},\n",
    "    \"ground truth\" : {\"mask_data\" : true_mask, \"class_labels\" : labels()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "universal-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
    "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "                 epochs: int = 100,\n",
    "                 epoch: int = 0,\n",
    "                 notebook: bool = False\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        progressbar = trange(self.epochs, desc='Progress')\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_DataLoader is not None:\n",
    "                self._validate()\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    self.lr_scheduler.batch(self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "            wandb.log({\"epoch\": self.epoch, \"train_loss\": self.training_loss[-1], \"val_loss\": self.validation_loss[-1]})\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            out = self.model(input)['out']  # one forward pass\n",
    "            loss = self.criterion(out, target)  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
    "\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "        mask_list=[]\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(input)['out']\n",
    "                loss = self.criterion(out, target)\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "\n",
    "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
    "            mask_list.append(wb_mask(input,out,target))\n",
    "\n",
    "\n",
    "        self.validation_losses.append(np.mean(valid_losses))\n",
    "                             \n",
    "        wandb.log({\"predictions\": mask_list})\n",
    "\n",
    "        batch_iter.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "swedish-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9db0a1c83b4591a89bf325e0c30de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6c212f78df4e81a45deed83575ca65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "mask_data must be a 2d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-0c935a3cddd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m                   notebook=True)\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# start training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtraining_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_rates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-81-80e6d53386a7>\u001b[0m in \u001b[0;36mrun_trainer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;34m\"\"\"Validation block\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_DataLoader\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;34m\"\"\"Learning rate scheduler block\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-80e6d53386a7>\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mbatch_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Validation: (loss {loss_value:.4f})'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mmask_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwb_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-237dc4518eaf>\u001b[0m in \u001b[0;36mwb_mask\u001b[1;34m(bg_img, pred_mask, true_mask)\u001b[0m\n\u001b[0;32m      2\u001b[0m   return wandb.Image(bg_img, masks={\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"prediction\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"mask_data\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mpred_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"class_labels\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \"ground truth\" : {\"mask_data\" : true_mask, \"class_labels\" : labels()}})\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_lightning\\lib\\site-packages\\wandb\\sdk\\data_types.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_or_path, mode, caption, grouping, classes, boxes, masks)\u001b[0m\n\u001b[0;32m   1576\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_from_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1578\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_initialization_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrouping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m     def _set_initialization_meta(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_lightning\\lib\\site-packages\\wandb\\sdk\\data_types.py\u001b[0m in \u001b[0;36m_set_initialization_meta\u001b[1;34m(self, grouping, caption, classes, boxes, masks)\u001b[0m\n\u001b[0;32m   1619\u001b[0m                     \u001b[0mmasks_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_item\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1620\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1621\u001b[1;33m                     \u001b[0mmasks_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageMask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1622\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmasks_final\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_lightning\\lib\\site-packages\\wandb\\sdk\\data_types.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, val, key)\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"class_labels\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1221\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1222\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_lightning\\lib\\site-packages\\wandb\\sdk\\data_types.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, val)\u001b[0m\n\u001b[0;32m   1295\u001b[0m             \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mask_data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m             if not (\n\u001b[0;32m   1299\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mask_data\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mask_data\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: mask_data must be a 2d array"
     ]
    }
   ],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "# model\n",
    "#model = UNet(in_channels=3,\n",
    "#             out_channels=2,\n",
    "#             n_blocks=4,\n",
    " #            start_filters=32,\n",
    "#             activation='relu',\n",
    "#             normalization='batch',\n",
    "#             conv_mode='same',\n",
    "#             dim=2).to(device)\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# trainer\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=1)\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_DataLoader=dataloader_training,\n",
    "                  validation_DataLoader=dataloader_validation,\n",
    "                  lr_scheduler=None,\n",
    "                  epochs=10,\n",
    "                  epoch=0,\n",
    "                  notebook=True)\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, Path('C:/Users/mm16jdc/Documents/CEDA_satellites/model_2021-03-22.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-compilation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
