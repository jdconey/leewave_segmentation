Due to IPython and Windows limitation, python multiprocessing isn't available now.
So `number_workers` is changed to 0 to avoid getting stuck
WandbCallback requires use of "SaveModelCallback" to log best model
WandbCallback was not able to prepare a DataLoader for logging prediction samples -> list index out of range
C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastprogress\fastprogress.py:74: UserWarning: Your generator is empty.
  warn("Your generator is empty.")
Setting-up type transforms pipelines
Collecting items from C:/Users/mm16jdc/Documents/CEDA_satellites/pytorch_mask_test/images_3/train/images
Found 491 items
2 datasets of sizes 491,0
Setting up Pipeline: PILBase.create
Setting up Pipeline: <lambda> -> PILBase.create

Building one sample
  Pipeline: PILBase.create
    starting from
      C:\Users\mm16jdc\Documents\CEDA_satellites\pytorch_mask_test\images_3\train\images\0.png
    applying PILBase.create gives
      PILImage mode=RGB size=400x400
  Pipeline: <lambda> -> PILBase.create
    starting from
      C:\Users\mm16jdc\Documents\CEDA_satellites\pytorch_mask_test\images_3\train\images\0.png
    applying <lambda> gives
      C:/Users/mm16jdc/Documents/CEDA_satellites/pytorch_mask_test/images_3/train/masks/0.png
    applying PILBase.create gives
      PILMask mode=L size=400x400

Final sample: (PILImage mode=RGB size=400x400, PILMask mode=L size=400x400)


Collecting items from C:/Users/mm16jdc/Documents/CEDA_satellites/pytorch_mask_test/images_3/train/images
Found 491 items
2 datasets of sizes 491,0
Setting up Pipeline: PILBase.create
Setting up Pipeline: <lambda> -> PILBase.create
Due to IPython and Windows limitation, python multiprocessing isn't available now.
So `number_workers` is changed to 0 to avoid getting stuck
Setting up after_item: Pipeline: AddMaskCodes -> ToTensor
Setting up before_batch: Pipeline: 
Setting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Flip -- {'size': (200, 200), 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} -> Normalize -- {'mean': tensor([[[[0.4850]],

         [[0.4560]],

         [[0.4060]]]]), 'std': tensor([[[[0.2290]],

         [[0.2240]],

         [[0.2250]]]]), 'axes': (0, 2, 3)}

Building one batch
Applying item_tfms to the first sample:
  Pipeline: AddMaskCodes -> ToTensor
    starting from
      (PILImage mode=RGB size=400x400, PILMask mode=L size=400x400)
    applying AddMaskCodes gives
      (PILImage mode=RGB size=400x400, PILMask mode=L size=400x400)
    applying ToTensor gives
      (TensorImage of size 3x400x400, TensorMask of size 400x400)

Adding the next 7 samples

No before_batch transform to apply

Collating items in a batch

Applying batch_tfms to the batch built
  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Flip -- {'size': (200, 200), 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} -> Normalize -- {'mean': tensor([[[[0.4850]],

         [[0.4560]],

         [[0.4060]]]]), 'std': tensor([[[[0.2290]],

         [[0.2240]],

         [[0.2250]]]]), 'axes': (0, 2, 3)}
    starting from
      (TensorImage of size 8x3x400x400, TensorMask of size 8x400x400)
    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives
      (TensorImage of size 8x3x400x400, TensorMask of size 8x400x400)
    applying Flip -- {'size': (200, 200), 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} gives
      (TensorImage of size 8x3x200x200, TensorMask of size 8x200x200)
    applying Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} gives
      (TensorImage of size 8x3x200x200, TensorMask of size 8x200x200)
    applying Normalize -- {'mean': tensor([[[[0.4850]],

         [[0.4560]],

         [[0.4060]]]]), 'std': tensor([[[[0.2290]],

         [[0.2240]],

         [[0.2250]]]]), 'axes': (0, 2, 3)} gives
      (TensorImage of size 8x3x200x200, TensorMask of size 8x200x200)
Due to IPython and Windows limitation, python multiprocessing isn't available now.
So `number_workers` is changed to 0 to avoid getting stuck
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\IPython\core\interactiveshell.py", line 3418, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-19-0ec4c0d2ec50>", line 2, in <module>
    learn.fit_flat_cos(10,slice(lr))
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\callback\schedule.py", line 135, in fit_flat_cos
    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 211, in fit
    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 160, in _with_events
    try: self(f'before_{event_type}');  f()
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 202, in _do_fit
    self._with_events(self._do_epoch, 'epoch', CancelEpochException)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 160, in _with_events
    try: self(f'before_{event_type}');  f()
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 196, in _do_epoch
    self._do_epoch_train()
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 188, in _do_epoch_train
    self._with_events(self.all_batches, 'train', CancelTrainException)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 160, in _with_events
    try: self(f'before_{event_type}');  f()
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 166, in all_batches
    for o in enumerate(self.dl): self.one_batch(*o)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 184, in one_batch
    self._with_events(self._do_one_batch, 'batch', CancelBatchException)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 160, in _with_events
    try: self(f'before_{event_type}');  f()
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\learner.py", line 177, in _do_one_batch
    self.loss_grad.backward()
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\torch\tensor.py", line 214, in backward
    return handle_torch_function(
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\torch\overrides.py", line 1060, in handle_torch_function
    result = overloaded_arg.__torch_function__(public_api, types, args, kwargs)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\fastai\torch_core.py", line 329, in __torch_function__
    res = super().__torch_function__(func, types, args=args, kwargs=kwargs)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\torch\tensor.py", line 995, in __torch_function__
    ret = func(*args, **kwargs)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\torch\tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\torch\autograd\__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\wandb\wandb_torch.py", line 285, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\IPython\core\interactiveshell.py", line 2045, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\IPython\core\ultratb.py", line 1170, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\IPython\core\ultratb.py", line 316, in wrapped
    return f(*args, **kwargs)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\IPython\core\ultratb.py", line 350, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\inspect.py", line 1503, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\inspect.py", line 1465, in getframeinfo
    lines, lnum = findsource(frame)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\site-packages\IPython\core\ultratb.py", line 170, in findsource
    file = getsourcefile(object) or getfile(object)
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\inspect.py", line 705, in getsourcefile
    if os.path.exists(filename):
  File "C:\ProgramData\Anaconda3\envs\fastai\lib\genericpath.py", line 19, in exists
    os.stat(path)
KeyboardInterrupt
Due to IPython and Windows limitation, python multiprocessing isn't available now.
So `number_workers` is changed to 0 to avoid getting stuck
